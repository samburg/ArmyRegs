{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import os\n",
    "import sys, getopt\n",
    "import shutil\n",
    "\n",
    "#this cell converts pdfs to text\n",
    "#if the pdf is a scanned image and needs its text extracted, it moves it to a new directory to be extracted in a new function\n",
    "\n",
    "\n",
    "#moves files that need to be extracted with tesseract into a new directory. enter your own directory here\n",
    "def moveImgFile(fname):\n",
    "    shutil.move(fname, \"/Users/samuelburgess/Documents/Jupyter/ARRegulations/pdf_textExtract2\")\n",
    "\n",
    "#converts pdf, returns its text content as a string\n",
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    infile = open(fname, 'rb')\n",
    "    for page in PDFPage.get_pages(infile, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    print(\"text\")\n",
    "    #print(text)\n",
    "    if \" \" not in text:\n",
    "        print(\"nottext\")\n",
    "        moveImgFile(fname)\n",
    "    return text\n",
    "\n",
    "#converts all pdfs in directory pdfDir, saves all resulting txt files to txtdir.\n",
    "#Repeatedly calls the first 'convert' function over each document in the directory\n",
    "def convertMultiple(pdfDir, txtDir):\n",
    "    if pdfDir == \"\": pdfDir = os.getcwd() + \"\\\\\" #if no pdfDir passed in \n",
    "    for pdf in os.listdir(pdfDir): #iterate through pdfs in pdf directory\n",
    "        fileExtension = pdf.split(\".\")[-1]\n",
    "        if fileExtension == \"pdf\":\n",
    "            pdfFilename = pdfDir + pdf \n",
    "            text = convert(pdfFilename) #get string of text content of pdf\n",
    "            textFilename = txtDir + pdf + \".txt\"\n",
    "            if not os.path.isfile(textFilename): \n",
    "                textFile = open(textFilename, \"w\") #make text file\n",
    "                textFile.write(text) #write text to text file\n",
    "pdfDir = \"ARReg2/\"\n",
    "txtDir = \"txt2/\"\n",
    "convertMultiple(pdfDir, txtDir)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the pdf to text converter doesn't extract any text, then we can use this function to extract the text using\n",
    "#pytesseract\n",
    "\n",
    "#all of the files that were shuttled to a new directory in the previous cell will have to be converted to a\n",
    "#.tiff file to proceed in this cell\n",
    "\n",
    "try:\n",
    "    from PIL import Image, ImageSequence\n",
    "except ImportError:\n",
    "    import Image\n",
    "import pytesseract\n",
    "from io import StringIO\n",
    "import os\n",
    "import glob\n",
    "import sys, getopt\n",
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "directory = 'tiff_textExtract2/*.tiff' #path\n",
    "txtDir = 'txt2'\n",
    "jpgFiles = glob.glob(directory)\n",
    "print(jpgFiles)\n",
    "\n",
    "# If you don't have tesseract executable in your PATH, include the following:\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/local/Cellar/tesseract/4.0.0/bin/tesseract'\n",
    "# Example tesseract_cmd = r'C:\\Program Files (x86)\\Tesseract-OCR\\tesseract'\n",
    "\n",
    "\n",
    "# Simple image to string\n",
    "for file in jpgFiles:\n",
    "    print(\"Processing file in image convert: \" + str(file))\n",
    "    image = Image.open(file)\n",
    "   # print(ImageSequence.Iterator(image)\n",
    "    imageText = \"\"\n",
    "    textFilename = str(file) + \".txt\"\n",
    "    for i, page in enumerate(ImageSequence.Iterator(image)):\n",
    "        print(\"page\", i)\n",
    "        if os.path.isfile(textFilename):\n",
    "            break\n",
    "        else:\n",
    "            imageText+=pytesseract.image_to_string(image)\n",
    "        #print(imageText)\n",
    "    if not os.path.isfile(textFilename): \n",
    "        #textFilename = \"/txt/\" + textFilename\n",
    "        textFile = open(textFilename, \"w\") #make text file\n",
    "        textFile.write(imageText) #write text to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writes all your text files to a dataframe\n",
    "\n",
    "import glob\n",
    "import errno\n",
    "import xlsxwriter\n",
    "import six\n",
    "import io\n",
    "from openpyxl import *\n",
    "from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "writer = pd.ExcelWriter('file.xlsx', engine='xlsxwriter')\n",
    "    \n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "\n",
    "columns = ['Doc']\n",
    "index=0\n",
    "x=0\n",
    "\n",
    "\n",
    "pathTXT = 'txt2/*.txt'\n",
    "filesTXT = glob.glob(pathTXT)\n",
    "\n",
    "raw_data = []\n",
    "file_names = []\n",
    "for name in filesTXT:\n",
    "    try:\n",
    "        with open(name, 'r') as myfile:\n",
    "            print(name)\n",
    "            data=myfile.read().replace('o\\n', ' ')\n",
    "            b = data.replace(' o ',  '')\n",
    "            c = b.replace('*', \"\")\n",
    "            d = c.replace('\\n', ' ')\n",
    "            e = d.replace('  ', ' ')\n",
    "            f = e.replace('â€¢', '')\n",
    "            h = f.replace('--', '-')\n",
    "            \n",
    "            file_names.append(name)\n",
    "            raw_data.append(h)\n",
    "        column_cell = 'A'  \n",
    "        #x+=1\n",
    "        #index+=1\n",
    "    except IOError as exc:\n",
    "        if exc.errno != errno.EISDIR:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "df = pd.DataFrame(raw_data, columns = ['Doc'])\n",
    "\n",
    "#wb.save(\"ARRegs.xlsx\")\n",
    "            \n",
    "#result_df = pd.read_excel('ARRegs.xlsx')\n",
    "#print(df)\n",
    "df.to_excel('ARRegulations2.xlsx', encoding='utf-8')\n",
    "result_df = pd.read_excel('ARRegulations2.xlsx')\n",
    "data_text=result_df['Doc']\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "print(df)\n",
    "\n",
    "            #word for line in open(words_file, 'r') for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#uncomment the commented line down there and comment the line above it to lemmatize the documents\n",
    "stemmer = PorterStemmer()\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "            #result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the docs\n",
    "processed_docs = df['Doc'].fillna('').astype(str).map(preprocess)\n",
    "processed_docs = df['Doc'].map(preprocess)\n",
    "processed_docs[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a 'dictionary' of all the words in the documents\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out all the words that are in over 50% of the documents\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_doc_9 = bow_corpus[9]\n",
    "for i in range(len(bow_doc_9)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_9[i][0], \n",
    "                                               dictionary[bow_doc_9[i][0]], \n",
    "bow_doc_9[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "#for doc in corpus_tfidf:\n",
    "    #pprint(doc)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the number of topics you want with the 'num_topics' argument\n",
    "#create your LDA model with this function\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=8, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=15, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model show\n",
    "for index, score in sorted(lda_model[bow_corpus[9]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf model show\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[9]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "unseen_document = 'chemical and nuclear demilitarizations'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "top_score = 0\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    if score > top_score:\n",
    "        print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 30)))\n",
    "        top_score = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the actual RN instead of the txt file name string\n",
    "for i in range(len(file_names)):\n",
    "    word = file_names[i].split('txt2/')\n",
    "    word = word[1].split('.')\n",
    "    file_names[i] = word[0]\n",
    "    print(word[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the number of pages in each pdf and output to an excel sheet\n",
    "from io import StringIO\n",
    "import os\n",
    "import glob\n",
    "import sys, getopt\n",
    "\n",
    "directory = 'ARReg2/*.pdf' #path\n",
    "pdfFiles = glob.glob(directory)\n",
    "pagedf = pd.DataFrame(columns = ['AR', 'pages'])\n",
    "\n",
    "pages = None\n",
    "j=0\n",
    "for doc in pdfFiles:\n",
    "    #output = StringIO()\n",
    "    #manager = PDFResourceManager()\n",
    "    #converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    #interpreter = PDFPageInterpreter(manager, converter)\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "    infile = open(doc, 'rb')\n",
    "    i=0\n",
    "    for page in PDFPage.get_pages(infile, pagenums):\n",
    "        #interpreter.process_page(page)\n",
    "        i+=1\n",
    "    word = doc.split('ARReg2/')\n",
    "    word = word[1].split('.')\n",
    "    doc = word[0]\n",
    "    #print(word[0])\n",
    "    print(doc)\n",
    "    pagedf.at[j, 'AR'] = doc\n",
    "    print(i)\n",
    "    pagedf.at[j, 'pages'] = i\n",
    "    #infile.close()\n",
    "    #converter.close()\n",
    "    #manager.close()\n",
    "    #output.close()\n",
    "    j+=1\n",
    "    #pagedf[:20]\n",
    "\n",
    "pagedf.to_excel('output3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import BigramCollocationFinder\n",
    "import os\n",
    "#import pdf\n",
    "import glob\n",
    "import sys, getopt\n",
    "from io import StringIO\n",
    "\n",
    "#directory = 'txt/*.txt' #path\n",
    "#words_file = glob.glob(directory)\n",
    "\n",
    "\n",
    "#break up into individual words\n",
    "#def read_words(words):\n",
    "#    return [word for line in open(words, 'r') for word in line.split()]\n",
    "\n",
    "\n",
    "#for file in words_file:\n",
    "def ngram(ARstring, i):\n",
    "    biGram = \"\"\n",
    "    fourGram = \"\"\n",
    "    nltk_tokens = nltk.word_tokenize(ARstring)\n",
    "    ARBigrams = (list(nltk.bigrams(nltk_tokens)))\n",
    "    ARWords = [word for word in nltk_tokens if word[0].isalpha()]\n",
    "    ARText = nltk.Text(ARWords)\n",
    "    \n",
    "#create bigrams from our ARTokens\n",
    "    ARBigramsFreqs = nltk.FreqDist(ARBigrams) # determine frequency of four-grams\n",
    "    for words, count in ARBigramsFreqs.most_common(10):\n",
    "        biGram += \" \".join(list(words)) + \"(\" + str(count) + \")\" + \", \"\n",
    "    #print(ARBigrams)\n",
    "    \n",
    "#create 4gram phrases  \n",
    "    AR4grams = list(nltk.ngrams(ARWords, 4)) # create four-grams\n",
    "    AR4gramsFreqs = nltk.FreqDist(AR4grams) # determine frequency of four-grams\n",
    "    for words, count in AR4gramsFreqs.most_common(10): # for the 10 most common four-grams\n",
    "        fourGram += \" \".join(list(words)) + \"(\" + str(count) + \")\" + \", \"\n",
    "        #final_df.at[i, 'Phrase_Count'] = count\n",
    "        #ARText.collocations()\n",
    "    final_df.at[i, 'Top_Bigrams'] = biGram\n",
    "    final_df.at[i, 'Top_Phrases'] = fourGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foreach processed_docs go through and compare assign a topic to each\n",
    "#write the topic and the filename from processed doc to excel sheet\n",
    "i=0\n",
    "final_df = pd.DataFrame(columns = ['AR', 'LDA_Topic', 'Top_Bigrams', 'Top_Phrases'])\n",
    "for doc in processed_docs:\n",
    "    LDA = \"\"\n",
    "    ARstring = \"\"\n",
    "    for word in doc:\n",
    "        ARstring += word + \" \"\n",
    "    ngram(ARstring, i)\n",
    "    bow_vector = dictionary.doc2bow(preprocess(ARstring))\n",
    "    #print(ARstring)\n",
    "    for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "        #print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 10)))\n",
    "        LDA += lda_model.print_topic(index, 15)\n",
    "    final_df.at[i, 'AR'] = file_names[i]\n",
    "    final_df.at[i, 'LDA_Topic'] = LDA\n",
    "    i+=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_excel('output4.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
